{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03106c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#실제 그래프를 그릴 수 있는 서브 함수\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#경고표시 생략(일시적으로 생략하는게 좋음) \n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#한글폰트 지정 \n",
    "import matplotlib.font_manager as fm\n",
    "font_name= fm.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "plt.rc(\"font\",family=font_name)\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "# 머신러닝을 위한 모듈 \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014581a",
   "metadata": {},
   "source": [
    "## 1. XOR 문제\n",
    "- 딥러닝의 첫번째 겨울"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd39cf3",
   "metadata": {},
   "source": [
    "### (1) OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc35a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[1]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e264716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4592/2035452072.py:1: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4592/2035452072.py:11: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4592/2035452072.py:14: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,3])#8,3 도 가능 \n",
    "y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3,1]),tf.float32,name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]),tf.float32,name='bias')\n",
    "\n",
    "# 가설 \n",
    "hypot = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "# 비용 \n",
    "cost= -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24888833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4592/413576794.py:1: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4592/413576794.py:2: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "가설 : [[0.4311003 ]\n",
      " [0.89502037]\n",
      " [0.8980473 ]\n",
      " [0.9900103 ]\n",
      " [0.8968121 ]\n",
      " [0.99019855]\n",
      " [0.9898766 ]\n",
      " [0.99912095]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5 ,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y),dtype=tf.float32))\n",
    "\n",
    "for step in range(1000):\n",
    "    _,h,p,a = sess.run([train,hypot,preds,accuracy],feed_dict={X:X_data,y:y_data})\n",
    "    \n",
    "print(\"가설 :\",h)\n",
    "print(\"예측 :\",p)\n",
    "print(\"정확도 :\",a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809d21e",
   "metadata": {},
   "source": [
    "### (2) AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a266ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0],[0],[0],[0],[0],[0],[0],[1]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718e0505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.00902706]\n",
      " [0.04549432]\n",
      " [0.04123598]\n",
      " [0.18369964]\n",
      " [0.04131526]\n",
      " [0.16907397]\n",
      " [0.18400025]\n",
      " [0.51565695]]\n",
      "예측 : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,3])#8,3 도 가능 \n",
    "y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3,1]),tf.float32,name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]),tf.float32,name='bias')\n",
    "\n",
    "# 가설 \n",
    "hypot = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "# 비용 \n",
    "cost= -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "# 세션 실해 \n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5 ,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y),dtype=tf.float32))\n",
    "\n",
    "for step in range(1000):\n",
    "    _,h,p,a = sess.run([train,hypot,preds,accuracy],feed_dict={X:X_data,y:y_data})\n",
    "    \n",
    "print(\"가설 :\",h)\n",
    "print(\"예측 :\",p)\n",
    "print(\"정확도 :\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050fdf5",
   "metadata": {},
   "source": [
    "### (3) XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8191196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91131131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.74380153]\n",
      " [0.7479044 ]\n",
      " [0.74734193]\n",
      " [0.751407  ]\n",
      " [0.7469548 ]\n",
      " [0.7504662 ]\n",
      " [0.7510241 ]\n",
      " [0.7544976 ]]\n",
      "예측 : [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,3])#8,3 도 가능 \n",
    "y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3,1]),tf.float32,name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]),tf.float32,name='bias')\n",
    "\n",
    "# 가설 \n",
    "hypot = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "# 비용 \n",
    "cost= -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "# 세션 실해 \n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5 ,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y),dtype=tf.float32))\n",
    "\n",
    "for step in range(1000):\n",
    "    _,h,p,a = sess.run([train,hypot,preds,accuracy],feed_dict={X:X_data,y:y_data})\n",
    "    \n",
    "print(\"가설 :\",h)\n",
    "print(\"예측 :\",p)\n",
    "print(\"정확도 :\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1628533",
   "metadata": {},
   "source": [
    "### (4) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec7527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea9c3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]]\n",
    "\n",
    "y = [0,1,1,1,1,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "009a3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100).fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60678134",
   "metadata": {},
   "outputs": [],
   "source": [
    "?svm.SVC # 파라미터 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb369f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# test 데이터 만들기 \n",
    "examples = [[0,0,0],[1,1,1],[0,1,1],[0,1,0],[1,0,0]]\n",
    "exam_label = [0,0,1,1,1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de09b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(exam_label,result)\n",
    "print(score)\n",
    "# svm도 XOR 잘하는데 인공신경망이 못했다 !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bc4cc",
   "metadata": {},
   "source": [
    "### (5) 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "527a622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.01378754]\n",
      " [0.99626434]\n",
      " [0.99636453]\n",
      " [0.99244034]\n",
      " [0.99698627]\n",
      " [0.9849942 ]\n",
      " [0.99102145]\n",
      " [0.03111184]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]],dtype=np.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,3])#8,3 도 가능 \n",
    "y = tf.placeholder(tf.float32,shape=[None,1]) # 출력의 갯수!!!! 얘로 정해지는 것\n",
    "\n",
    "# 첫번째 hidden layer: 입력은 조정 못하지만 출력은 변경 가능 \n",
    "W1 = tf.Variable(tf.random.normal([3,10]),tf.float32,name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([10]),tf.float32,name='bias1')\n",
    "hypot1 =tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "\n",
    "# 두번째 hidden layer : 다음에도 히든 레이어가 나오면 마음대로 출력 갯수를 해도 되지만 마지막이라면 1로 해야함\n",
    "W2 = tf.Variable(tf.random.normal([10,1]),tf.float32,name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([1]),tf.float32,name='bias2')\n",
    "hypot =tf.sigmoid(tf.matmul(hypot1,W2)+b2) # 첫번째 만든 가설을 다듬어 주는 \n",
    "\n",
    "\n",
    "# 비용 \n",
    "cost= -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "# 세션 실해 \n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5 ,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y),dtype=tf.float32))\n",
    "\n",
    "for step in range(10000):\n",
    "    _,h,p,a = sess.run([train,hypot,preds,accuracy],feed_dict={X:X_data,y:y_data})\n",
    "    \n",
    "print(\"가설 :\",h)\n",
    "print(\"예측 :\",p)\n",
    "print(\"정확도 :\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c069ba9e",
   "metadata": {},
   "source": [
    "입력\n",
    "히든- 학습이되는 \n",
    "출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c7dfa",
   "metadata": {},
   "source": [
    "### (6) Deep & Wide \n",
    "\n",
    "+ Deep : 6개의 hidden layer\n",
    "+ Wide : 각 계층의 입출력 갯수는 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d38c40cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[6.0963631e-04]\n",
      " [9.9981904e-01]\n",
      " [9.9980950e-01]\n",
      " [9.9959677e-01]\n",
      " [9.9978393e-01]\n",
      " [9.9972343e-01]\n",
      " [9.9966347e-01]\n",
      " [9.7283721e-04]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]],dtype=np.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,3])#8,3 도 가능 \n",
    "y = tf.placeholder(tf.float32,shape=[None,1]) # 출력의 갯수!!!! 얘로 정해지는 것\n",
    "\n",
    "# 첫번째 hidden layer: 입력은 조정 못하지만 출력은 변경 가능 \n",
    "W1 = tf.Variable(tf.random.normal([3,50]),tf.float32)\n",
    "b1 = tf.Variable(tf.random.normal([50]),tf.float32)\n",
    "layer1 =tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# 두번째 hidden layer\n",
    "W2 = tf.Variable(tf.random.normal([50,50]),tf.float32)\n",
    "b2 = tf.Variable(tf.random.normal([50]),tf.float32)\n",
    "layer2 =tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "# 세번째 hidden layer\n",
    "W3 = tf.Variable(tf.random.normal([50,50]),tf.float32)\n",
    "b3 = tf.Variable(tf.random.normal([50]),tf.float32)\n",
    "layer3 =tf.sigmoid(tf.matmul(layer2,W3)+b3)\n",
    "\n",
    "# 네번째 hidden layer\n",
    "W4 = tf.Variable(tf.random.normal([50,50]),tf.float32)\n",
    "b4 = tf.Variable(tf.random.normal([50]),tf.float32)\n",
    "layer4 =tf.sigmoid(tf.matmul(layer3,W4)+b4)\n",
    "\n",
    "# 다섯번째 hidden layer\n",
    "W5 = tf.Variable(tf.random.normal([50,50]),tf.float32)\n",
    "b5 = tf.Variable(tf.random.normal([50]),tf.float32)\n",
    "layer5 =tf.sigmoid(tf.matmul(layer4,W5)+b5)\n",
    "\n",
    "# 여섯번째 hidden layer \n",
    "W6 = tf.Variable(tf.random.normal([50,1]),tf.float32)\n",
    "b6 = tf.Variable(tf.random.normal([1]),tf.float32)\n",
    "hypot =tf.sigmoid(tf.matmul(layer5,W6)+b6) # 첫번째 만든 가설을 다듬어 주는 \n",
    "\n",
    "# 비용 \n",
    "cost= -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "# 세션 실해 \n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5 ,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y),dtype=tf.float32))\n",
    "\n",
    "for step in range(10000):\n",
    "    _,h,p,a = sess.run([train,hypot,preds,accuracy],feed_dict={X:X_data,y:y_data})\n",
    "    \n",
    "print(\"가설 :\",h)\n",
    "print(\"예측 :\",p)\n",
    "print(\"정확도 :\",a)\n",
    "\n",
    "# 결과 같이 훠얼~~씬 더 안정된 결과값이 나타남. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fdf37",
   "metadata": {},
   "source": [
    "## 2. Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685c4618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.21078002]\n",
      " [0.93953043]\n",
      " [0.93857914]\n",
      " [0.99644023]\n",
      " [0.94818556]\n",
      " [0.99405557]\n",
      " [0.99652624]\n",
      " [0.998689  ]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,3]) \n",
    "y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random.normal([10]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random.normal([1]), tf.float32, name=\"bias2\")\n",
    "hypot = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "\n",
    "cost= -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5 ,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y),dtype=tf.float32))\n",
    "\n",
    "for step in range(1000):\n",
    "    _,h,p,a = sess.run([train,hypot,preds,accuracy],feed_dict={X:X_data,y:y_data})\n",
    "    \n",
    "print(\"가설 :\",h)\n",
    "print(\"예측 :\",p)\n",
    "print(\"정확도 :\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0592e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.02748516]\n",
      " [0.9935593 ]\n",
      " [0.9932144 ]\n",
      " [0.9891668 ]\n",
      " [0.9957013 ]\n",
      " [0.9874074 ]\n",
      " [0.9851136 ]\n",
      " [0.03267312]]\n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "# 첫번째 hidden layer\n",
    "W1 = tf.Variable(tf.random.normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random.normal([10]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 hidden layer\n",
    "W2 = tf.Variable(tf.random.normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random.normal([1]), tf.float32, name=\"bias2\")\n",
    "hypot = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "tf.summary.histogram(\"weight2\", W2)\n",
    "\n",
    "# 비용\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "# 최소비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# 세션 실행\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a, m = sess.run([train, hypot, preds, accuracy, merged_summary], feed_dict={X:X_data, y:y_data})\n",
    "    writer.add_summary(m, global_step=step)\n",
    "    \n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3542d9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 : [[0.01060531]\n",
      " [0.9988489 ]\n",
      " [0.99583006]\n",
      " [0.9868938 ]\n",
      " [0.99796647]\n",
      " [0.9885674 ]\n",
      " [0.9798701 ]\n",
      " [0.04020432]]\n",
      "예측 : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X_data = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,1,0],[1,0,1],[1,1,1]],dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]],dtype=np.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,3])#8,3 도 가능 \n",
    "y = tf.placeholder(tf.float32,shape=[None,1]) # 출력의 갯수!!!! 얘로 정해지는 것\n",
    "\n",
    "# 첫번째 hidden layer: 입력은 조정 못하지만 출력은 변경 가능 \n",
    "with tf.name_scope(\"layer\"):\n",
    "    W1 = tf.Variable(tf.random.normal([3,10]),tf.float32,name='weight1')\n",
    "    b1 = tf.Variable(tf.random.normal([10]),tf.float32,name='bias1')\n",
    "    hypot1 =tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "    \n",
    "    tf.summary.histogram('weight',W1)\n",
    "    tf.summary.histogram('bias1',b1)\n",
    "    tf.summary.histogram('hypot1',hypot1)\n",
    "\n",
    "# 두번째 hidden layer : 다음에도 히든 레이어가 나오면 마음대로 출력 갯수를 해도 되지만 마지막이라면 1로 해야함\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random.normal([10,1]),tf.float32,name='weight2')\n",
    "    b2 = tf.Variable(tf.random.normal([1]),tf.float32,name='bias2')\n",
    "    hypot =tf.sigmoid(tf.matmul(hypot1,W2)+b2) # 첫번째 만든 가설을 다듬어 주는 \n",
    "    \n",
    "    tf.summary.histogram('weight',W2)\n",
    "    tf.summary.histogram('bias1',b2)\n",
    "    tf.summary.histogram('hypot1',hypot)\n",
    "\n",
    "\n",
    "# 비용 \n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost= -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "    tf.summary.scalar('cost',cost)\n",
    "\n",
    "    \n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    preds = tf.cast(hypot > 0.5 ,dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y),dtype=tf.float32))\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "# 세션 실해 \n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _,h,p,a,m = sess.run([train,hypot,preds,accuracy,merged_summary],feed_dict={X:X_data,y:y_data})\n",
    "    writer.add_summary(m, global_step=step)\n",
    "    \n",
    "print(\"가설 :\",h)\n",
    "print(\"예측 :\",p)\n",
    "print(\"정확도 :\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1076b01d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.73234594]\n",
      " [0.7604754 ]\n",
      " [0.735226  ]\n",
      " [0.72805715]\n",
      " [0.8076594 ]\n",
      " [0.79713947]\n",
      " [0.75278544]\n",
      " [0.6914733 ]]\n",
      "예측 :  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 :  0.75\n"
     ]
    }
   ],
   "source": [
    "##### tensorbord _rata -> 이름 변경\n",
    "##### tnsorboard --loggit=log_dir2\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "# 첫번째 hidden layer\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random.normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random.normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"hypot1\", hypot1)\n",
    "\n",
    "# 두번째 hidden layer\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random.normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random.normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"hypot\", hypot)\n",
    "\n",
    "# 비용\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "# 최소비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    preds = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)    \n",
    "\n",
    "# 세션 실행\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"log_dir2/alpha001\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a, m = sess.run([train, hypot, preds, accuracy, merged_summary], \n",
    "                             feed_dict={X:X_data, y:y_data})\n",
    "    writer.add_summary(m, global_step=step)\n",
    "    \n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914babcb",
   "metadata": {},
   "source": [
    "## 3. ReLU:Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430289c",
   "metadata": {},
   "source": [
    "+ ReLU :큰 값에 대한 제한을 뽑아 내자 --> sigmoid를 개선, 그전엔 1 이상 넘어가지 못하게 제한을 둠\n",
    "+ 최대값을 맘껏 나타낼 수 있게 \n",
    "+ sigmoid 자체가 없어지는 것이 아님 히든 레이어에서 쓰는 것이고 마지막에는 시그모이드 써야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7307c5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_26196/924815153.py:2: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_26196/924815153.py:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777) # 항상 같은 결과가 나올 수 있게 \n",
    "mnist = input_data.read_data_sets('data/mnist/',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343ec6d",
   "metadata": {},
   "source": [
    "### (1) 단순 모델 : 89.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274f1738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_24616/3713787720.py:13: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,28*28])\n",
    "y = tf.placeholder(tf.int32,shape=[None,10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28,10])) # 원핫 인코딩으로 바꾸고 계산해야함.\n",
    "b = tf.Variable(tf.random_normal([10])) \n",
    "\n",
    "# 가설, 비용\n",
    "logit = tf.matmul(X,W)+b\n",
    "hypot = tf.nn.softmax(logit)\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,labels=y)\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8b03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=tf.argmax(hypot,1)\n",
    "correct=tf.equal(preds,tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 2.8358313636346284\n",
      "274 1.062052041834051\n",
      "274 0.838500477075577\n",
      "274 0.7331850410591472\n",
      "274 0.6692519315806302\n",
      "274 0.624872704094107\n",
      "274 0.5912235124002801\n",
      "274 0.5641030131686815\n",
      "274 0.5416709137504752\n",
      "274 0.5228132875399156\n",
      "274 0.5068879948962823\n",
      "274 0.49256115615367896\n",
      "274 0.48001735974441867\n",
      "274 0.46890798146074464\n",
      "274 0.45885969573801205\n",
      "274 0.44965183859521707\n",
      "274 0.4420980801365593\n",
      "274 0.4339840704202652\n",
      "274 0.4270284786549485\n",
      "274 0.4202925090356306\n",
      "274 0.4144443166797813\n",
      "274 0.40862749609080234\n",
      "274 0.40364830678159536\n",
      "274 0.39844519092278025\n",
      "274 0.39363614721731716\n",
      "274 0.3893297735127534\n",
      "274 0.3855479147488421\n",
      "274 0.38131989533251\n",
      "274 0.37729393888603546\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, tf.reduce_mean(cost)], \n",
    "                        feed_dict={X:batch_x, y:batch_y})\n",
    "        \n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(i, avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(accuracy,feed_dict={X:mnist.train.images,\n",
    "                             y:mnist.train.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e501081",
   "metadata": {},
   "source": [
    "### (2) 딥러닝 모델 \n",
    "\n",
    "+ Deep : layer는 7개 \n",
    "+ Wide : 계층간 입출력 갯수는 256개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226755bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_21220/2112842872.py:1: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_21220/2112842872.py:5: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_21220/2112842872.py:50: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,28*28])\n",
    "y = tf.placeholder(tf.int32,shape=[None,10])\n",
    "\n",
    "# Layer 1 \n",
    "W1 = tf.Variable(tf.random_normal([28*28,256])) \n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "logit1 = tf.matmul(X,W1)+b1\n",
    "layer1= tf.nn.sigmoid(logit1)\n",
    "\n",
    "# Layer 2\n",
    "W2 = tf.Variable(tf.random_normal([256,256])) \n",
    "b2 = tf.Variable(tf.random_normal([256])) \n",
    "logit2 = tf.matmul(layer1,W2)+b2\n",
    "layer2= tf.nn.sigmoid(logit2)\n",
    "\n",
    "# Layer 3 \n",
    "W3 = tf.Variable(tf.random_normal([256,256])) \n",
    "b3 = tf.Variable(tf.random_normal([256])) \n",
    "logit3 = tf.matmul(layer2,W3)+b3\n",
    "layer3= tf.nn.sigmoid(logit3)\n",
    "\n",
    "# Layer 4 \n",
    "W4 = tf.Variable(tf.random_normal([256,256])) \n",
    "b4 = tf.Variable(tf.random_normal([256])) \n",
    "logit4 = tf.matmul(layer3,W4)+b4\n",
    "layer4= tf.nn.sigmoid(logit4)\n",
    "\n",
    "# Layer 5 \n",
    "W5 = tf.Variable(tf.random_normal([256,256])) \n",
    "b5 = tf.Variable(tf.random_normal([256])) \n",
    "logit5 = tf.matmul(layer4,W5)+b5\n",
    "layer5= tf.nn.sigmoid(logit5)\n",
    "\n",
    "# Layer 6 \n",
    "W6 = tf.Variable(tf.random_normal([256,256])) \n",
    "b6 = tf.Variable(tf.random_normal([256])) \n",
    "logit6 = tf.matmul(layer5,W6)+b6\n",
    "layer6= tf.nn.sigmoid(logit6) \n",
    "\n",
    "# Layer 7 \n",
    "W7 = tf.Variable(tf.random_normal([256,10])) \n",
    "b7 = tf.Variable(tf.random_normal([10])) \n",
    "logit7 = tf.matmul(layer6,W7)+b7 # 비용계산을 위한 \n",
    "hypot =tf.nn.softmax(logit7) # 예측을 위한 \n",
    "\n",
    "\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit7,labels=y)\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09f6895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_21220/3348449370.py:5: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_21220/3348449370.py:6: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "109 173.04606073552918\n",
      "109 167.9138366005637\n",
      "109 165.93171927712183\n",
      "109 162.50793658169832\n",
      "109 159.20121647227896\n",
      "109 159.19129812067206\n",
      "109 159.69580424915668\n",
      "109 161.07446406971326\n",
      "109 162.76514892578123\n",
      "109 158.98768497813828\n"
     ]
    }
   ],
   "source": [
    "preds=tf.argmax(hypot,1)\n",
    "correct=tf.equal(preds,tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 10\n",
    "batch_size = 500\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, tf.reduce_mean(cost)], \n",
    "                        feed_dict={X:batch_x, y:batch_y})\n",
    "        \n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(i, avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ee9ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09649091"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(accuracy,feed_dict={X:mnist.train.images,\n",
    "                             y:mnist.train.labels})\n",
    "# 9.6 % --> 너무 작네???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13a611d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 로 바꾸자  --> 다 바꾸면 이게 값이 너무 커져서 발산이 될 수 있음(nan)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,28*28])\n",
    "y = tf.placeholder(tf.int32,shape=[None,10])\n",
    "\n",
    "# Layer 1 \n",
    "W1 = tf.Variable(tf.random_normal([28*28,256])) \n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "logit1 = tf.matmul(X,W1)+b1\n",
    "layer1= tf.nn.relu(logit1)\n",
    "\n",
    "# Layer 2\n",
    "W2 = tf.Variable(tf.random_normal([256,256])) \n",
    "b2 = tf.Variable(tf.random_normal([256])) \n",
    "logit2 = tf.matmul(layer1,W2)+b2\n",
    "layer2= tf.nn.relu(logit2)\n",
    "\n",
    "# Layer 3 \n",
    "W3 = tf.Variable(tf.random_normal([256,256])) \n",
    "b3 = tf.Variable(tf.random_normal([256])) \n",
    "logit3 = tf.matmul(layer2,W3)+b3\n",
    "layer3= tf.nn.sigmoid(logit3)\n",
    "\n",
    "# Layer 4 \n",
    "W4 = tf.Variable(tf.random_normal([256,256])) \n",
    "b4 = tf.Variable(tf.random_normal([256])) \n",
    "logit4 = tf.matmul(layer3,W4)+b4\n",
    "layer4= tf.nn.sigmoid(logit4)\n",
    "\n",
    "# Layer 5 \n",
    "W5 = tf.Variable(tf.random_normal([256,256])) \n",
    "b5 = tf.Variable(tf.random_normal([256])) \n",
    "logit5 = tf.matmul(layer4,W5)+b5\n",
    "layer5= tf.nn.sigmoid(logit5)\n",
    "\n",
    "# Layer 6 \n",
    "W6 = tf.Variable(tf.random_normal([256,256])) \n",
    "b6 = tf.Variable(tf.random_normal([256])) \n",
    "logit6 = tf.matmul(layer5,W6)+b6\n",
    "layer6= tf.nn.sigmoid(logit6) \n",
    "\n",
    "# Layer 7 \n",
    "W7 = tf.Variable(tf.random_normal([256,10])) \n",
    "b7 = tf.Variable(tf.random_normal([10])) \n",
    "logit7 = tf.matmul(layer6,W7)+b7 # 비용계산을 위한 \n",
    "hypot =tf.nn.softmax(logit7) # 예측을 위한 \n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit7,labels=y))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b050e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.235724434852601\n",
      "2 1.4050799560546863\n",
      "3 1.2280180384896016\n",
      "4 1.1190754266218699\n",
      "5 1.0332024346698405\n",
      "6 0.9294734703410749\n",
      "7 0.8727383867177098\n",
      "8 0.8326425812461165\n",
      "9 0.8181207403269684\n",
      "10 0.7923475211316889\n",
      "11 0.7581104144183068\n",
      "12 0.7435910092700616\n",
      "13 0.7257834733616221\n",
      "14 0.6913659904219891\n",
      "15 0.6593522653796453\n",
      "16 0.6778004877133803\n",
      "17 0.6687837338447571\n",
      "18 0.6195468701015819\n",
      "19 0.598629583878951\n",
      "20 0.599892783598467\n",
      "21 0.5712485588680617\n",
      "22 0.5413877725601194\n",
      "23 0.5319650471210483\n",
      "24 0.5599426658587026\n",
      "25 0.5479447897997771\n",
      "26 0.515264802954414\n",
      "27 0.4947388211163605\n",
      "28 0.49216614625670657\n",
      "29 0.4885391192002727\n",
      "30 0.5044579256664624\n"
     ]
    }
   ],
   "source": [
    "preds=tf.argmax(hypot,1)\n",
    "correct=tf.equal(preds,tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], \n",
    "                        feed_dict={X:batch_x, y:batch_y})\n",
    "        \n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(epoch+1, avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93abfaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8526545"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(accuracy,feed_dict={X:mnist.train.images,\n",
    "                             y:mnist.train.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b83e40",
   "metadata": {},
   "source": [
    "### (3) Xavier 초기화\n",
    "-  초기값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "221fcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# ReLU 로 바꾸자  --> 다 바꾸면 이게 값이 너무 커져서 발산이 될 수 있음(nan)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,28*28])\n",
    "y = tf.placeholder(tf.int32,shape=[None,10])\n",
    "\n",
    "# Layer 1 \n",
    "W1 = tf.get_variable('W1',shape=[784,256],initializer=tf.contrib.layers.xavier_initializer()) # 랜덤으로 초기화 했는데 ->Xavier로\n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "logit1 = tf.matmul(X,W1)+b1\n",
    "layer1= tf.nn.relu(logit1)\n",
    "\n",
    "# Layer 2\n",
    "W2 = tf.get_variable('W2',shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256])) \n",
    "logit2 = tf.matmul(layer1,W2)+b2\n",
    "layer2= tf.nn.relu(logit2)\n",
    "\n",
    "# Layer 3 \n",
    "W3 = tf.get_variable('W3',shape=[256,256],initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b3 = tf.Variable(tf.random_normal([256])) \n",
    "logit3 = tf.matmul(layer2,W3)+b3\n",
    "layer3= tf.nn.sigmoid(logit3)\n",
    "\n",
    "# Layer 4 \n",
    "W4 = tf.get_variable('W4',shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([256])) \n",
    "logit4 = tf.matmul(layer3,W4)+b4\n",
    "layer4= tf.nn.sigmoid(logit4)\n",
    "\n",
    "# Layer 5 \n",
    "W5 = tf.get_variable('W5',shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([256])) \n",
    "logit5 = tf.matmul(layer4,W5)+b5\n",
    "layer5= tf.nn.sigmoid(logit5)\n",
    "\n",
    "# Layer 6 \n",
    "W6 = tf.get_variable('W6',shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([256])) \n",
    "logit6 = tf.matmul(layer5,W6)+b6\n",
    "layer6= tf.nn.sigmoid(logit6) \n",
    "\n",
    "# Layer 7 \n",
    "W7 = tf.get_variable('W7',shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([10])) \n",
    "logit7 = tf.matmul(layer6,W7)+b7 # 비용계산을 위한 \n",
    "hypot =tf.nn.softmax(logit7) # 예측을 위한 \n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit7,labels=y))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7019ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.3176589142192485\n",
      "2 2.3144896940751507\n",
      "3 2.313154565637762\n",
      "4 2.3122891382737603\n",
      "5 2.311018893501977\n",
      "6 2.3090184393796065\n",
      "7 2.3083708737113264\n",
      "8 2.3080394918268388\n",
      "9 2.306741706674747\n",
      "10 2.3058424914966933\n",
      "11 2.303660781166772\n",
      "12 2.3024149946732937\n",
      "13 2.2960041245547202\n",
      "14 2.234600150368429\n",
      "15 1.8385872710834852\n",
      "16 1.7242085166410974\n",
      "17 1.5893667108362384\n",
      "18 1.4428060978109183\n",
      "19 1.1684622328931629\n",
      "20 0.908508892059326\n",
      "21 0.7099640180847865\n",
      "22 0.5920330277356236\n",
      "23 0.5083220026709818\n",
      "24 0.44795145338231884\n",
      "25 0.3751917538859626\n",
      "26 0.32708082372492026\n",
      "27 0.26384068028493357\n",
      "28 0.23652806431055065\n",
      "29 0.20753414094448097\n",
      "30 0.18931744445453993\n",
      "31 0.1697220923142\n",
      "32 0.15720632779327307\n",
      "33 0.18346668365326804\n",
      "34 0.13169151104309332\n",
      "35 0.11961472622372894\n",
      "36 0.12390000617639584\n",
      "37 0.09806571048091751\n",
      "38 0.088384576107968\n",
      "39 0.0825980931791392\n",
      "40 0.07535422608256337\n",
      "41 0.06928455667739566\n",
      "42 0.0627395043725317\n",
      "43 0.0567749840359796\n",
      "44 0.05125525324182077\n",
      "45 0.04793425949459728\n",
      "46 0.041020511476831015\n",
      "47 0.10008815315975383\n",
      "48 0.05489669575948603\n",
      "49 0.03362455733120442\n",
      "50 0.029709262024949907\n"
     ]
    }
   ],
   "source": [
    "preds=tf.argmax(hypot,1)\n",
    "correct=tf.equal(preds,tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], \n",
    "                        feed_dict={X:batch_x, y:batch_y})\n",
    "        \n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(epoch+1, avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "461314e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9954182"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(accuracy,feed_dict={X:mnist.train.images,\n",
    "                             y:mnist.train.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61ee2a",
   "metadata": {},
   "source": [
    "### (4) Dropout\n",
    "+ 시간도 절약,과적합 막을 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b68c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# ReLU 로 바꾸자  --> 다 바꾸면 이게 값이 너무 커져서 발산이 될 수 있음(nan)\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=[None,28*28])\n",
    "y = tf.placeholder(tf.int32,shape=[None,10])\n",
    "\n",
    "prob= tf.placeholder(tf.float32) # 몇퍼센트 유지할 건지\n",
    "\n",
    "# Layer 1 \n",
    "W1 = tf.get_variable('W1',shape=[784,512],initializer=tf.contrib.layers.xavier_initializer()) # 랜덤으로 초기화 했는데 ->Xavier로\n",
    "b1 = tf.Variable(tf.random_normal([512])) \n",
    "logit1 = tf.matmul(X,W1)+b1\n",
    "layer1= tf.nn.relu(logit1)\n",
    "layer1= tf.nn.dropout(layer1,keep_prob=prob)\n",
    "\n",
    "# Layer 2\n",
    "W2 = tf.get_variable('W2',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512])) \n",
    "logit2 = tf.matmul(layer1,W2)+b2\n",
    "layer2= tf.nn.relu(logit2)\n",
    "layer2= tf.nn.dropout(layer2,keep_prob=prob)\n",
    "\n",
    "# Layer 3 \n",
    "W3 = tf.get_variable('W3',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer()) \n",
    "b3 = tf.Variable(tf.random_normal([512])) \n",
    "logit3 = tf.matmul(layer2,W3)+b3\n",
    "layer3= tf.nn.sigmoid(logit3)\n",
    "\n",
    "# Layer 4 \n",
    "W4 = tf.get_variable('W4',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512])) \n",
    "logit4 = tf.matmul(layer3,W4)+b4\n",
    "layer4= tf.nn.sigmoid(logit4)\n",
    "layer4= tf.nn.dropout(layer4,keep_prob=prob)\n",
    "\n",
    "# Layer 5 \n",
    "W5 = tf.get_variable('W5',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512])) \n",
    "logit5 = tf.matmul(layer4,W5)+b5\n",
    "layer5= tf.nn.sigmoid(logit5)\n",
    "layer5= tf.nn.dropout(layer5,keep_prob=prob)\n",
    "\n",
    "# Layer 6 \n",
    "W6 = tf.get_variable('W6',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512])) \n",
    "logit6 = tf.matmul(layer5,W6)+b6\n",
    "layer6= tf.nn.sigmoid(logit6) \n",
    "layer6= tf.nn.dropout(layer6,keep_prob=prob)\n",
    "\n",
    "# Layer 7 \n",
    "W7 = tf.get_variable('W7',shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([10])) \n",
    "logit7 = tf.matmul(layer6,W7)+b7 # 비용계산을 위한 \n",
    "hypot =tf.nn.softmax(logit7) # 예측을 위한 \n",
    "hypot= tf.nn.dropout(hypot,keep_prob=prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit7,labels=y))\n",
    "\n",
    "# 최소비용 \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa3009d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.420636810822921\n",
      "2 2.339529213471845\n",
      "3 2.3260858657143313\n",
      "4 2.3185423062064423\n",
      "5 2.316306960365988\n",
      "6 2.31202662381259\n",
      "7 2.312394416982478\n",
      "8 2.3111447316950007\n",
      "9 2.310579654520208\n",
      "10 2.3103304914994673\n",
      "11 2.309570380991156\n",
      "12 2.3095888866077767\n",
      "13 2.309569921493529\n",
      "14 2.308238822763617\n",
      "15 2.308062041889537\n",
      "16 2.3073662159659665\n",
      "17 2.307659202922474\n",
      "18 2.306977552067151\n",
      "19 2.304973708933051\n",
      "20 2.30352877443487\n",
      "21 2.301192978078669\n",
      "22 2.291674560200085\n",
      "23 2.2451035291498367\n",
      "24 2.0009349034049286\n",
      "25 1.8179924574765287\n",
      "26 1.6586795815554525\n",
      "27 1.5515913213383055\n",
      "28 1.4976713848114025\n",
      "29 1.446486775224859\n",
      "30 1.3920659802176754\n",
      "31 1.3421087260679767\n",
      "32 1.292340413440357\n",
      "33 1.2401884898272424\n",
      "34 1.1982359404997402\n",
      "35 1.1593141958930264\n",
      "36 1.1219120621681218\n",
      "37 1.0845943318713802\n",
      "38 1.0504346784678367\n",
      "39 1.0196313218636943\n",
      "40 0.989716885740107\n",
      "41 0.9598858653415339\n",
      "42 0.9322869576107371\n",
      "43 0.9078526024384934\n",
      "44 0.8822144534371112\n",
      "45 0.845125797661868\n",
      "46 0.8200802352211697\n",
      "47 0.786885648207231\n",
      "48 0.7246226358413698\n",
      "49 0.6210070060599936\n",
      "50 0.5265586310083218\n"
     ]
    }
   ],
   "source": [
    "preds=tf.argmax(hypot,1)\n",
    "correct=tf.equal(preds,tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], \n",
    "                        feed_dict={X:batch_x, y:batch_y,prob:0.7}) # 확률 전달\n",
    "        \n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(epoch+1, avg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4fdec08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25936362"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(accuracy,feed_dict={X:mnist.train.images,\n",
    "                             y:mnist.train.labels,prob:1.0})\n",
    "# 훈련은 70%만 하고 테스트는 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e7a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d070e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c48cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00439195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
